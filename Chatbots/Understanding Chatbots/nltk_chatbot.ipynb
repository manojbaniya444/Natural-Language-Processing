{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chatbot: NLTK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import nltk\n",
    "import string\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "f = open(\"corpus_restaurant.txt\", \"r\")\n",
    "corpus = f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"Welcome to Gourmet Haven, the best dining destination in the heart of the city. Our restaurant offers an exquisite menu featuring a wide range of dishes that cater to all taste buds. Whether you're a \""
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "corpus[:200]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Text Preprocessing Step"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lowercase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to C:\\Users\\acer/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to C:\\Users\\acer/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package omw-1.4 to C:\\Users\\acer/nltk_data...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "raw_doc = corpus.lower()\n",
    "nltk.download('punkt')\n",
    "nltk.download('wordnet')\n",
    "nltk.download(\"omw-1.4\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Tokenizing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence_tokens = nltk.sent_tokenize(raw_doc)\n",
    "word_tokens = nltk.word_tokenize(raw_doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome to gourmet haven, the best dining destination in the heart of the city.',\n",
       " 'our restaurant offers an exquisite menu featuring a wide range of dishes that cater to all taste buds.',\n",
       " \"whether you're a fan of italian cuisine, love asian flavors, or prefer classic american dishes, we have something for everyone!\",\n",
       " 'our chefs use only the freshest ingredients to create mouthwatering meals.',\n",
       " 'from appetizers to desserts, each dish is crafted with care and precision.',\n",
       " 'our vegan options are not only healthy but also delicious!',\n",
       " 'we have gluten-free dishes to accommodate our guests with dietary restrictions.',\n",
       " 'be sure to try our specials today; they are always a hit.',\n",
       " 'looking to book a table?',\n",
       " 'you can make a reservation for two or more with ease.']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sentence_tokens[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['welcome',\n",
       " 'to',\n",
       " 'gourmet',\n",
       " 'haven',\n",
       " ',',\n",
       " 'the',\n",
       " 'best',\n",
       " 'dining',\n",
       " 'destination',\n",
       " 'in']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_tokens[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Lemmatizing and Normalizing text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "lemmer = nltk.WordNetLemmatizer()\n",
    "\n",
    "def LemTokens(tokens):\n",
    "    return [lemmer.lemmatize(token) for token in tokens]\n",
    "\n",
    "remove_punc_dict = dict((ord(punct), None) for punct in string.punctuation)\n",
    "\n",
    "def LemNormalize(text):\n",
    "    return LemTokens(nltk.word_tokenize(text.lower().translate(remove_punc_dict)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Greet User"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "greet_inputs = (\"hello\", \"hi\", \"greetings\", \"sup\", \"what's up\", \"hey\",)\n",
    "greet_responses = [\"hi\", \"hey\", \"*nods*\", \"hi there\", \"hello\", \"I am glad! You are talking to me\"]\n",
    "\n",
    "def greet(sentence):\n",
    "    for word in sentence.split():\n",
    "        if word.lower() in greet_inputs:\n",
    "            return random.choice(greet_responses)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Response from bot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def response(user_response):\n",
    "    \"\"\"Expected to be the user input\"\"\"\n",
    "    # hold the response generated by the bot\n",
    "    robo1_response = \"\"\n",
    "    \n",
    "    # tfidf object created which will convert a collection of raw documents to a matrix of TF-IDF features\n",
    "    TfidfVector = TfidfVectorizer(tokenizer=LemNormalize, stop_words=\"english\")\n",
    "    \n",
    "    # tfidf called upon sentence tokens which is a list of sentences (including the user response which is at last appended)\n",
    "    tfidf = TfidfVector.fit_transform(sentence_tokens)\n",
    "    \n",
    "    # The cosine similarity between the last TF-IDF vector (representing the user response) and the rest of the vectors stored in vals\n",
    "    vals = cosine_similarity(tfidf[-1], tfidf)\n",
    "    \n",
    "    # The argsort method returns the indices that world sort the array. The index of the second highest similarity value (excluding the last entry, which would be the user's response itself and is 1).\n",
    "    idx = vals.argsort()[0][-2]\n",
    "    \n",
    "    # The similarity vales array is flattened to a 1D array\n",
    "    flat = vals.flatten()\n",
    "    \n",
    "    # The array is sorted in ascending order (smallest cosinge value to highest cosine value)\n",
    "    flat.sort()\n",
    "    \n",
    "    # The second highest similarity value (the highest being the users response itself) is stored in req_tfidf\n",
    "    req_tfidf = flat[-2]\n",
    "    \n",
    "    # to check if the similarity score is 0: if 0 no sentence is similar to user query.\n",
    "    if(req_tfidf == 0):\n",
    "        robo1_response = robo1_response + \"I am sorry! I don't understand you\"\n",
    "        return robo1_response\n",
    "    else:\n",
    "        robo1_response = robo1_response + sentence_tokens[idx]\n",
    "        return robo1_response"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ChatFlow:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Restaurant Bot: I am a Restaurant Bot. I will answer your queries about restaurants. If you want to exit, type exit!\n",
      "You:  Hello\n",
      "Restaurant Bot: *nods*\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:525: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
      "  warnings.warn(\n",
      "C:\\Users\\acer\\AppData\\Roaming\\Python\\Python312\\site-packages\\sklearn\\feature_extraction\\text.py:408: UserWarning: Your stop_words may be inconsistent with your preprocessing. Tokenizing the stop words generated tokens ['ha', 'le', 'u', 'wa'] not in stop_words.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You:  where is the location\n",
      "Restaurant Bot: our location is easily accessible, with ample parking available.\n",
      "You:  what about safety\n",
      "Restaurant Bot: safety is a top priority at gourmet haven.\n",
      "You:  who is the chef\n",
      "Restaurant Bot: our chefs' specials change daily, ensuring there's always something new to try.\n",
      "You:  what are special items\n",
      "Restaurant Bot: for special occasions, our private dining area is perfect.\n",
      "You:  bye\n",
      "Restaurant Bot: I am sorry! I don't understand you\n",
      "You:  exit\n",
      "Restaurant Bot: Bye! Take care!\n"
     ]
    }
   ],
   "source": [
    "# Flag true to keep conversation going\n",
    "flag = True\n",
    "\n",
    "# Initial bot message\n",
    "print(\"Restaurant Bot: I am a Restaurant Bot. I will answer your queries about restaurants. If you want to exit, type exit!\")\n",
    "\n",
    "while(flag):\n",
    "    # getting the user response\n",
    "    user_response = input()\n",
    "    print(\"You: \", user_response)\n",
    "    user_response = user_response.lower()\n",
    "    \n",
    "    # if user response is other than 'exit',\n",
    "    if(user_response != \"exit\"):\n",
    "        # if user response is thanks or thank you, the bot will stop the conversation\n",
    "        if(user_response == \"thanks\" or user_response == \"thank you\"):\n",
    "            flag = False\n",
    "            print(\"Restaurant Bot: You are welcome!\")\n",
    "        else:\n",
    "            # check if the response from user is in greet or not, if in greet then greet the user.\n",
    "            if(greet(user_response) != None):\n",
    "                print(\"Restaurant Bot: \" + greet(user_response))\n",
    "            # otherwise perform the response function to answer the query (retrieve the most similar sentence from the corpus above)\n",
    "            else:\n",
    "                # add the user input to the sentence tokens at last\n",
    "                sentence_tokens.append(user_response)\n",
    "                \n",
    "                # also add the word tokens in the word tokens list\n",
    "                word_tokens = word_tokens + nltk.word_tokenize(user_response)\n",
    "                \n",
    "                # get unique vocab (words) including the latest user input query.\n",
    "                final_words = list(set(word_tokens))\n",
    "                \n",
    "                # Prepare for the response.\n",
    "                print(\"Restaurant Bot: \", end=\"\")\n",
    "                \n",
    "                # get the most similar sentence from the corpus.\n",
    "                print(response(user_response))\n",
    "                \n",
    "                # remove the user response from the sentence token collection.\n",
    "                sentence_tokens.remove(user_response)\n",
    "    else:\n",
    "        flag = False\n",
    "        print(\"Restaurant Bot: Bye! Take care!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
