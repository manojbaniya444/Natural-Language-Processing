{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Character-level recurrent sequence to sequence model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import keras\n",
    "import os\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 64\n",
    "EPOCHS = 100\n",
    "LATENT_DIM = 256\n",
    "NUM_SAMPLES = 10000\n",
    "DATA_PATH = \"./data/fra.txt\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparing the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_texts = []\n",
    "target_texts = []\n",
    "input_characters = set()\n",
    "target_characters = set()\n",
    "\n",
    "with open(DATA_PATH, \"r\", encoding=\"utf-8\") as f:\n",
    "    lines = f.read().split(\"\\n\")\n",
    "    \n",
    "for line in lines[: min(NUM_SAMPLES, len(lines) -1)]:\n",
    "    input_text, target_text, _ = line.split(\"\\t\")\n",
    "    \n",
    "    # using \"tab\" as the start sequence and \"\\n\" as the end sequence in text.\n",
    "    \n",
    "    target_text = \"\\t\" + target_text + \"\\n\"\n",
    "\n",
    "    input_texts.append(input_text)\n",
    "    target_texts.append(target_text)\n",
    "    \n",
    "    # building a character level vocabulary\n",
    "    for char in input_text:\n",
    "        if char not in input_characters:\n",
    "            input_characters.add(char)\n",
    "            \n",
    "    for char in target_text:\n",
    "        if char not in target_characters:\n",
    "            target_characters.add(char)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(['Go.', 'Go.', 'Go.', 'Go.', 'Hi.', 'Hi.', 'Run!', 'Run!', 'Run!', 'Run!'],\n",
       " ['\\tVa !\\n',\n",
       "  '\\tMarche.\\n',\n",
       "  '\\tEn route !\\n',\n",
       "  '\\tBouge !\\n',\n",
       "  '\\tSalut !\\n',\n",
       "  '\\tSalut.\\n',\n",
       "  '\\tCours\\u202f!\\n',\n",
       "  '\\tCourez\\u202f!\\n',\n",
       "  '\\tPrenez vos jambes à vos cous !\\n',\n",
       "  '\\tFile !\\n'])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[:10], target_texts[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept: sorted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2, 5, 5, 6, 9]\n",
      "Sorted words by length: ['fig', 'date', 'apple', 'grape', 'banana', 'cherry']\n",
      "Sorted dictionary by value: [('apple', 1), ('date', 3), ('fig', 5), ('grape', 6), ('banana', 10), ('cherry', 93)]\n"
     ]
    }
   ],
   "source": [
    "# concpet: sorted\n",
    "numbers = [5, 2, 9, 1, 5, 6]\n",
    "sorted_numbers = sorted(numbers)\n",
    "print(sorted_numbers)\n",
    "\n",
    "words = [\"apple\", \"banana\", \"cherry\", \"date\", \"fig\", \"grape\"]\n",
    "sorted_words = sorted(words, key=len)\n",
    "print(f\"Sorted words by length: {sorted_words}\")\n",
    "\n",
    "dictionary = {\"apple\": 1, \"banana\": 10, \"cherry\": 93, \"date\": 3, \"fig\": 5, \"grape\": 6}\n",
    "sorted_dictionary = sorted(dictionary.items(), key=lambda x: x[1])\n",
    "print(f\"Sorted dictionary by value: {sorted_dictionary}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_characters = sorted(list(input_characters))\n",
    "target_characters = sorted(list(target_characters))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_encoder_tokens = len(input_characters)\n",
    "num_decoder_tokens = len(target_characters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_encoder_seq_length = max([len(x) for x in input_texts])\n",
    "max_decoder_seq_length = max([len(x) for x in target_texts])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of samples: 10000\n",
      "Number of unique input tokens: 70\n",
      "Number of unique output tokens: 91\n",
      "Maximum sequence length for inputs: 14\n",
      "Maximum sequence length for outputs: 59\n"
     ]
    }
   ],
   "source": [
    "print(f\"Number of samples: {len(input_texts)}\")\n",
    "print(f\"Number of unique input tokens: {num_encoder_tokens}\")\n",
    "print(f\"Number of unique output tokens: {num_decoder_tokens}\")\n",
    "print(f\"Maximum sequence length for inputs: {max_encoder_seq_length}\")\n",
    "print(f\"Maximum sequence length for outputs: {max_decoder_seq_length}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_encoder_seq_length, num_encoder_tokens), dtype=\"float32\"\n",
    ") # to make shape (10000, 14, 70)\n",
    "\n",
    "decoder_input_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")\n",
    "\n",
    "decoder_target_data = np.zeros(\n",
    "    (len(input_texts), max_decoder_seq_length, num_decoder_tokens), dtype=\"float32\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 14, 70), (10000, 59, 91), (10000, 59, 91))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data.shape, decoder_input_data.shape, decoder_target_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "       0., 0.], dtype=float32)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_token_index = dict([(char, i) for i, char in enumerate(input_characters)])\n",
    "target_token_index = dict([(char, i) for i, char in enumerate(target_characters)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 0., 0., 1.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.],\n",
       "        [0., 0., 0., 0.]]])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = np.zeros(shape=(2,3,4))\n",
    "a[0,0,3] = 1\n",
    "a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concept ohe and for else:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[[0., 1., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [1., 0., 0., 0., 0.]],\n",
       "\n",
       "       [[0., 0., 1., 0., 0.],\n",
       "        [0., 0., 0., 1., 0.],\n",
       "        [0., 0., 0., 0., 1.]]])"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sample_words = [\"a\", \"bc\", \"bcd\"]\n",
    "sample_vocab = {\" \": 0, \"a\": 1,\"b\": 2, \"c\": 3, \"d\": 4}\n",
    "ohe = np.zeros(shape=(len(sample_words), 3, 5)) # ohe for 3 words where for one word max length is 3 and vocab size of data is 4\n",
    "\n",
    "for index, word in enumerate(sample_words):\n",
    "    for t, character in enumerate(word):\n",
    "        ohe[index, t, sample_vocab[character]] = 1.0\n",
    "        \n",
    "    ohe[index, t + 1 :, sample_vocab[\" \"]] = 1.0\n",
    "    \n",
    "ohe # [1, 0, 0, 0, 0] is space  [0, 1, 0, 0, 0] is a, [0, 0, 1, 0, 0] is b, [0, 0, 0, 1, 0] is c, [0, 0, 0, 0, 1] is d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i, (input_text, target_text) in enumerate(zip(input_texts, target_texts)):\n",
    "    for t, char in enumerate(input_text):\n",
    "        encoder_input_data[i, t, input_token_index[char]] = 1.0\n",
    "    encoder_input_data[i, t + 1 :, input_token_index[\" \"]] = 1.0\n",
    "    \n",
    "    for t, char in enumerate(target_text):\n",
    "        # decoder_target_data is ahead of decoder_input_data by one timestep\n",
    "        decoder_input_data[i, t, target_token_index[char]] = 1.0\n",
    "        if t > 0:\n",
    "            # decoder_target_data will be ahead by one timestep\n",
    "            # and will not include the start character.\n",
    "            decoder_target_data[i, t - 1, target_token_index[char]] = 1.0\n",
    "            \n",
    "    decoder_input_data[i, t + 1 :, target_token_index[\" \"]] = 1.0\n",
    "    decoder_target_data[i, t:, target_token_index[\" \"]] = 1.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Go.'"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.],\n",
       "       [1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.]], dtype=float32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "encoder_input_data[0] # first input for encoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\tVa !\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_texts[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " '\\t')"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_input_data[0][0], target_characters[np.argmax(decoder_input_data[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 1., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0.], dtype=float32),\n",
       " 'V')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "decoder_target_data[0][0], target_characters[np.argmax(decoder_target_data[0][0])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' '"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_characters[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('\\t', '\\n', ' ')"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_characters[0], target_characters[1], target_characters[2]\n",
    "# 0 : start token\n",
    "# 1 : end token\n",
    "# 2 : space for padding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input character: \t, Target character: V\n",
      "Input character: V, Target character: a\n",
      "Input character: a, Target character:  \n",
      "Input character:  , Target character: !\n",
      "Input character: !, Target character: \n",
      "\n",
      "Input character: \n",
      ", Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n",
      "Input character:  , Target character:  \n"
     ]
    }
   ],
   "source": [
    "for decoder_input, decoder_target in zip(decoder_input_data[0], decoder_target_data[0]):\n",
    "    print(f\"Input character: {target_characters[np.argmax(decoder_input)]}, Target character: {target_characters[np.argmax(decoder_target)]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Go.', '\\tVa !\\n')"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_texts[0], target_texts[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = keras.Input(shape=(None, num_encoder_tokens))\n",
    "encoder = keras.layers.LSTM(LATENT_DIM, return_state=True)\n",
    "# output from the encoder is not needed, only the states are needed\n",
    "encoder_outputs, state_hidden, state_cell = encoder(encoder_inputs)\n",
    "\n",
    "encoder_states = [state_hidden, state_cell]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = keras.Input(shape=(None, num_decoder_tokens))\n",
    "\n",
    "decoder_lstm = keras.layers.LSTM(LATENT_DIM, return_sequences=True, return_state=True)\n",
    "decoder_outputs, _, _ = decoder_lstm(decoder_inputs, initial_state=encoder_states)\n",
    "decoder_dense = keras.layers.Dense(num_decoder_tokens, activation=\"softmax\")\n",
    "decoder_outputs = decoder_dense(decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = keras.Model([encoder_inputs, decoder_inputs], decoder_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃<span style=\"font-weight: bold\"> Layer (type)        </span>┃<span style=\"font-weight: bold\"> Output Shape      </span>┃<span style=\"font-weight: bold\">    Param # </span>┃<span style=\"font-weight: bold\"> Connected to      </span>┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">70</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>)  │          <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                 │\n",
       "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)         │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">334,848</span> │ input_layer[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>] │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>),      │            │                   │\n",
       "│                     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)       │ [(<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,     │    <span style=\"color: #00af00; text-decoration-color: #00af00\">356,352</span> │ input_layer_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]… │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">1</span>],       │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>), (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>,      │            │ lstm[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">2</span>]        │\n",
       "│                     │ <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">91</span>)  │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,387</span> │ lstm_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n",
       "</pre>\n"
      ],
      "text/plain": [
       "┏━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┓\n",
       "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)       \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape     \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m   Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to     \u001b[0m\u001b[1m \u001b[0m┃\n",
       "┡━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━┩\n",
       "│ input_layer         │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m70\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ input_layer_1       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m91\u001b[0m)  │          \u001b[38;5;34m0\u001b[0m │ -                 │\n",
       "│ (\u001b[38;5;33mInputLayer\u001b[0m)        │                   │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)         │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),     │    \u001b[38;5;34m334,848\u001b[0m │ input_layer[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m] │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m),      │            │                   │\n",
       "│                     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)]      │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)       │ [(\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m,     │    \u001b[38;5;34m356,352\u001b[0m │ input_layer_1[\u001b[38;5;34m0\u001b[0m]… │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m1\u001b[0m],       │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m), (\u001b[38;5;45mNone\u001b[0m,      │            │ lstm[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m2\u001b[0m]        │\n",
       "│                     │ \u001b[38;5;34m256\u001b[0m)]             │            │                   │\n",
       "├─────────────────────┼───────────────────┼────────────┼───────────────────┤\n",
       "│ dense (\u001b[38;5;33mDense\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m91\u001b[0m)  │     \u001b[38;5;34m23,387\u001b[0m │ lstm_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
       "└─────────────────────┴───────────────────┴────────────┴───────────────────┘\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">714,587</span> (2.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m714,587\u001b[0m (2.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">714,587</span> (2.73 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m714,587\u001b[0m (2.73 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(\n",
    "    optimizer=\"rmsprop\",\n",
    "    loss=\"categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 87ms/step - accuracy: 0.7051 - loss: 1.5626 - val_accuracy: 0.7147 - val_loss: 1.0956\n",
      "Epoch 2/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7458 - loss: 0.9596 - val_accuracy: 0.7332 - val_loss: 0.9549\n",
      "Epoch 3/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 80ms/step - accuracy: 0.7629 - loss: 0.8591 - val_accuracy: 0.7515 - val_loss: 0.8689\n",
      "Epoch 4/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 83ms/step - accuracy: 0.7885 - loss: 0.7634 - val_accuracy: 0.7813 - val_loss: 0.7779\n",
      "Epoch 5/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 85ms/step - accuracy: 0.8013 - loss: 0.7021 - val_accuracy: 0.7862 - val_loss: 0.7303\n",
      "Epoch 6/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.8137 - loss: 0.6429 - val_accuracy: 0.7997 - val_loss: 0.6864\n",
      "Epoch 7/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - accuracy: 0.8216 - loss: 0.6113 - val_accuracy: 0.8032 - val_loss: 0.6675\n",
      "Epoch 8/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.8284 - loss: 0.5861 - val_accuracy: 0.8092 - val_loss: 0.6467\n",
      "Epoch 9/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 92ms/step - accuracy: 0.8336 - loss: 0.5656 - val_accuracy: 0.8175 - val_loss: 0.6289\n",
      "Epoch 10/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.8382 - loss: 0.5502 - val_accuracy: 0.8260 - val_loss: 0.5997\n",
      "Epoch 11/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 90ms/step - accuracy: 0.8443 - loss: 0.5292 - val_accuracy: 0.8285 - val_loss: 0.5885\n",
      "Epoch 12/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.8485 - loss: 0.5171 - val_accuracy: 0.8290 - val_loss: 0.5878\n",
      "Epoch 13/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 91ms/step - accuracy: 0.8533 - loss: 0.5012 - val_accuracy: 0.8366 - val_loss: 0.5637\n",
      "Epoch 14/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 93ms/step - accuracy: 0.8572 - loss: 0.4886 - val_accuracy: 0.8404 - val_loss: 0.5510\n",
      "Epoch 15/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 96ms/step - accuracy: 0.8604 - loss: 0.4759 - val_accuracy: 0.8426 - val_loss: 0.5448\n",
      "Epoch 16/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.8641 - loss: 0.4627 - val_accuracy: 0.8451 - val_loss: 0.5324\n",
      "Epoch 17/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.8654 - loss: 0.4569 - val_accuracy: 0.8462 - val_loss: 0.5244\n",
      "Epoch 18/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.8683 - loss: 0.4452 - val_accuracy: 0.8469 - val_loss: 0.5234\n",
      "Epoch 19/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.8706 - loss: 0.4364 - val_accuracy: 0.8497 - val_loss: 0.5116\n",
      "Epoch 20/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.8730 - loss: 0.4302 - val_accuracy: 0.8498 - val_loss: 0.5084\n",
      "Epoch 21/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.8758 - loss: 0.4199 - val_accuracy: 0.8513 - val_loss: 0.5059\n",
      "Epoch 22/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 106ms/step - accuracy: 0.8774 - loss: 0.4120 - val_accuracy: 0.8550 - val_loss: 0.4936\n",
      "Epoch 23/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 121ms/step - accuracy: 0.8796 - loss: 0.4074 - val_accuracy: 0.8561 - val_loss: 0.4895\n",
      "Epoch 24/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.8802 - loss: 0.4019 - val_accuracy: 0.8571 - val_loss: 0.4849\n",
      "Epoch 25/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.8825 - loss: 0.3952 - val_accuracy: 0.8581 - val_loss: 0.4807\n",
      "Epoch 26/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.8847 - loss: 0.3877 - val_accuracy: 0.8599 - val_loss: 0.4762\n",
      "Epoch 27/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.8872 - loss: 0.3796 - val_accuracy: 0.8613 - val_loss: 0.4762\n",
      "Epoch 28/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 113ms/step - accuracy: 0.8887 - loss: 0.3731 - val_accuracy: 0.8623 - val_loss: 0.4688\n",
      "Epoch 29/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 115ms/step - accuracy: 0.8897 - loss: 0.3691 - val_accuracy: 0.8630 - val_loss: 0.4673\n",
      "Epoch 30/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.8911 - loss: 0.3660 - val_accuracy: 0.8633 - val_loss: 0.4661\n",
      "Epoch 31/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - accuracy: 0.8930 - loss: 0.3593 - val_accuracy: 0.8651 - val_loss: 0.4605\n",
      "Epoch 32/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 115ms/step - accuracy: 0.8945 - loss: 0.3524 - val_accuracy: 0.8659 - val_loss: 0.4583\n",
      "Epoch 33/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 114ms/step - accuracy: 0.8951 - loss: 0.3499 - val_accuracy: 0.8659 - val_loss: 0.4575\n",
      "Epoch 34/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 116ms/step - accuracy: 0.8974 - loss: 0.3437 - val_accuracy: 0.8669 - val_loss: 0.4558\n",
      "Epoch 35/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 116ms/step - accuracy: 0.8991 - loss: 0.3376 - val_accuracy: 0.8676 - val_loss: 0.4515\n",
      "Epoch 36/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.8999 - loss: 0.3339 - val_accuracy: 0.8687 - val_loss: 0.4506\n",
      "Epoch 37/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9017 - loss: 0.3291 - val_accuracy: 0.8687 - val_loss: 0.4521\n",
      "Epoch 38/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9039 - loss: 0.3217 - val_accuracy: 0.8693 - val_loss: 0.4457\n",
      "Epoch 39/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 110ms/step - accuracy: 0.9046 - loss: 0.3173 - val_accuracy: 0.8692 - val_loss: 0.4467\n",
      "Epoch 40/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9058 - loss: 0.3158 - val_accuracy: 0.8701 - val_loss: 0.4461\n",
      "Epoch 41/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9072 - loss: 0.3099 - val_accuracy: 0.8712 - val_loss: 0.4425\n",
      "Epoch 42/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 118ms/step - accuracy: 0.9087 - loss: 0.3053 - val_accuracy: 0.8719 - val_loss: 0.4426\n",
      "Epoch 43/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9096 - loss: 0.3018 - val_accuracy: 0.8713 - val_loss: 0.4421\n",
      "Epoch 44/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.9103 - loss: 0.2982 - val_accuracy: 0.8713 - val_loss: 0.4458\n",
      "Epoch 45/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 119ms/step - accuracy: 0.9124 - loss: 0.2940 - val_accuracy: 0.8715 - val_loss: 0.4437\n",
      "Epoch 46/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 108ms/step - accuracy: 0.9128 - loss: 0.2906 - val_accuracy: 0.8736 - val_loss: 0.4407\n",
      "Epoch 47/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - accuracy: 0.9141 - loss: 0.2857 - val_accuracy: 0.8733 - val_loss: 0.4414\n",
      "Epoch 48/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9159 - loss: 0.2813 - val_accuracy: 0.8722 - val_loss: 0.4430\n",
      "Epoch 49/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9165 - loss: 0.2791 - val_accuracy: 0.8740 - val_loss: 0.4392\n",
      "Epoch 50/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9170 - loss: 0.2756 - val_accuracy: 0.8727 - val_loss: 0.4441\n",
      "Epoch 51/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.9190 - loss: 0.2695 - val_accuracy: 0.8740 - val_loss: 0.4420\n",
      "Epoch 52/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9190 - loss: 0.2697 - val_accuracy: 0.8736 - val_loss: 0.4437\n",
      "Epoch 53/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9216 - loss: 0.2625 - val_accuracy: 0.8743 - val_loss: 0.4444\n",
      "Epoch 54/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9213 - loss: 0.2617 - val_accuracy: 0.8730 - val_loss: 0.4483\n",
      "Epoch 55/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 101ms/step - accuracy: 0.9232 - loss: 0.2557 - val_accuracy: 0.8750 - val_loss: 0.4417\n",
      "Epoch 56/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9239 - loss: 0.2522 - val_accuracy: 0.8742 - val_loss: 0.4464\n",
      "Epoch 57/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9252 - loss: 0.2488 - val_accuracy: 0.8748 - val_loss: 0.4426\n",
      "Epoch 58/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9259 - loss: 0.2474 - val_accuracy: 0.8758 - val_loss: 0.4451\n",
      "Epoch 59/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9271 - loss: 0.2430 - val_accuracy: 0.8752 - val_loss: 0.4469\n",
      "Epoch 60/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9272 - loss: 0.2406 - val_accuracy: 0.8769 - val_loss: 0.4464\n",
      "Epoch 61/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 113ms/step - accuracy: 0.9291 - loss: 0.2349 - val_accuracy: 0.8759 - val_loss: 0.4476\n",
      "Epoch 62/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9302 - loss: 0.2322 - val_accuracy: 0.8755 - val_loss: 0.4480\n",
      "Epoch 63/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m15s\u001b[0m 117ms/step - accuracy: 0.9308 - loss: 0.2304 - val_accuracy: 0.8766 - val_loss: 0.4478\n",
      "Epoch 64/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9319 - loss: 0.2277 - val_accuracy: 0.8767 - val_loss: 0.4516\n",
      "Epoch 65/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.9329 - loss: 0.2258 - val_accuracy: 0.8754 - val_loss: 0.4534\n",
      "Epoch 66/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9338 - loss: 0.2203 - val_accuracy: 0.8767 - val_loss: 0.4534\n",
      "Epoch 67/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9341 - loss: 0.2192 - val_accuracy: 0.8758 - val_loss: 0.4599\n",
      "Epoch 68/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9355 - loss: 0.2159 - val_accuracy: 0.8755 - val_loss: 0.4574\n",
      "Epoch 69/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9360 - loss: 0.2135 - val_accuracy: 0.8765 - val_loss: 0.4590\n",
      "Epoch 70/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9371 - loss: 0.2085 - val_accuracy: 0.8778 - val_loss: 0.4571\n",
      "Epoch 71/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9379 - loss: 0.2072 - val_accuracy: 0.8763 - val_loss: 0.4629\n",
      "Epoch 72/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9390 - loss: 0.2038 - val_accuracy: 0.8764 - val_loss: 0.4647\n",
      "Epoch 73/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.9395 - loss: 0.1998 - val_accuracy: 0.8766 - val_loss: 0.4695\n",
      "Epoch 74/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9406 - loss: 0.1985 - val_accuracy: 0.8768 - val_loss: 0.4629\n",
      "Epoch 75/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9412 - loss: 0.1962 - val_accuracy: 0.8765 - val_loss: 0.4699\n",
      "Epoch 76/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 106ms/step - accuracy: 0.9424 - loss: 0.1920 - val_accuracy: 0.8768 - val_loss: 0.4708\n",
      "Epoch 77/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9425 - loss: 0.1905 - val_accuracy: 0.8767 - val_loss: 0.4747\n",
      "Epoch 78/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 106ms/step - accuracy: 0.9433 - loss: 0.1884 - val_accuracy: 0.8769 - val_loss: 0.4723\n",
      "Epoch 79/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9438 - loss: 0.1868 - val_accuracy: 0.8751 - val_loss: 0.4750\n",
      "Epoch 80/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 109ms/step - accuracy: 0.9446 - loss: 0.1848 - val_accuracy: 0.8764 - val_loss: 0.4765\n",
      "Epoch 81/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9463 - loss: 0.1795 - val_accuracy: 0.8776 - val_loss: 0.4802\n",
      "Epoch 82/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9465 - loss: 0.1779 - val_accuracy: 0.8765 - val_loss: 0.4853\n",
      "Epoch 83/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9463 - loss: 0.1784 - val_accuracy: 0.8763 - val_loss: 0.4822\n",
      "Epoch 84/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 111ms/step - accuracy: 0.9474 - loss: 0.1753 - val_accuracy: 0.8763 - val_loss: 0.4827\n",
      "Epoch 85/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9491 - loss: 0.1697 - val_accuracy: 0.8769 - val_loss: 0.4864\n",
      "Epoch 86/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9487 - loss: 0.1705 - val_accuracy: 0.8769 - val_loss: 0.4904\n",
      "Epoch 87/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 103ms/step - accuracy: 0.9497 - loss: 0.1681 - val_accuracy: 0.8766 - val_loss: 0.4946\n",
      "Epoch 88/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 106ms/step - accuracy: 0.9500 - loss: 0.1651 - val_accuracy: 0.8762 - val_loss: 0.4959\n",
      "Epoch 89/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9515 - loss: 0.1618 - val_accuracy: 0.8761 - val_loss: 0.4967\n",
      "Epoch 90/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 107ms/step - accuracy: 0.9514 - loss: 0.1604 - val_accuracy: 0.8756 - val_loss: 0.4992\n",
      "Epoch 91/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 105ms/step - accuracy: 0.9521 - loss: 0.1588 - val_accuracy: 0.8760 - val_loss: 0.4996\n",
      "Epoch 92/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 104ms/step - accuracy: 0.9525 - loss: 0.1574 - val_accuracy: 0.8755 - val_loss: 0.5013\n",
      "Epoch 93/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 100ms/step - accuracy: 0.9536 - loss: 0.1538 - val_accuracy: 0.8766 - val_loss: 0.5044\n",
      "Epoch 94/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 99ms/step - accuracy: 0.9541 - loss: 0.1523 - val_accuracy: 0.8762 - val_loss: 0.5063\n",
      "Epoch 95/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.9536 - loss: 0.1526 - val_accuracy: 0.8759 - val_loss: 0.5122\n",
      "Epoch 96/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 100ms/step - accuracy: 0.9556 - loss: 0.1473 - val_accuracy: 0.8747 - val_loss: 0.5170\n",
      "Epoch 97/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m12s\u001b[0m 100ms/step - accuracy: 0.9559 - loss: 0.1448 - val_accuracy: 0.8763 - val_loss: 0.5184\n",
      "Epoch 98/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 100ms/step - accuracy: 0.9558 - loss: 0.1457 - val_accuracy: 0.8757 - val_loss: 0.5130\n",
      "Epoch 99/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 102ms/step - accuracy: 0.9566 - loss: 0.1429 - val_accuracy: 0.8755 - val_loss: 0.5202\n",
      "Epoch 100/100\n",
      "\u001b[1m125/125\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m14s\u001b[0m 112ms/step - accuracy: 0.9576 - loss: 0.1410 - val_accuracy: 0.8758 - val_loss: 0.5242\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x21d6be4ad80>"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(\n",
    "    [encoder_input_data, decoder_input_data],\n",
    "    decoder_target_data,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    epochs=EPOCHS,\n",
    "    validation_split=0.2\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Inference\n",
    "- Encode Input and get the inigial decoder state from the encoder layer\n",
    "- Run one step of decoder with this initial state and input as a \"start sequence\" in our case \"\\t\"\n",
    "- Repeat with the current target token and current states"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save(\"eng2fre.keras\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "encoder_inputs = model.input[0]  # input_1\n",
    "encoder_outputs, state_h_enc, state_c_enc = model.layers[2].output  # lstm_1\n",
    "encoder_states = [state_h_enc, state_c_enc]\n",
    "encoder_model = keras.Model(encoder_inputs, encoder_states)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "decoder_inputs = model.input[1]  # input_2\n",
    "decoder_state_input_h = keras.Input(shape=(LATENT_DIM,))\n",
    "decoder_state_input_c = keras.Input(shape=(LATENT_DIM,))\n",
    "decoder_states_inputs = [decoder_state_input_h, decoder_state_input_c]\n",
    "decoder_lstm = model.layers[3]\n",
    "decoder_outputs, state_h_dec, state_c_dec = decoder_lstm(\n",
    "    decoder_inputs, initial_state=decoder_states_inputs\n",
    ")\n",
    "decoder_states = [state_h_dec, state_c_dec]\n",
    "decoder_dense = model.layers[4]\n",
    "decoder_outputs = decoder_dense(decoder_outputs)\n",
    "decoder_model = keras.Model(\n",
    "    [decoder_inputs] + decoder_states_inputs, [decoder_outputs] + decoder_states\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "reverse_input_char_index = dict((i, char) for char, i in input_token_index.items())\n",
    "reverse_target_char_index = dict((i, char) for char, i in target_token_index.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode_sequence(input_seq):\n",
    "    # Encode the input as state vectors.\n",
    "    states_value = encoder_model.predict(input_seq, verbose=0)\n",
    "\n",
    "    # Generate empty target sequence of length 1.\n",
    "    target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "    # Populate the first character of target sequence with the start character.\n",
    "    target_seq[0, 0, target_token_index[\"\\t\"]] = 1.0\n",
    "\n",
    "    # Sampling loop for a batch of sequences\n",
    "    # (to simplify, here we assume a batch of size 1).\n",
    "    stop_condition = False\n",
    "    decoded_sentence = \"\"\n",
    "    while not stop_condition:\n",
    "        output_tokens, h, c = decoder_model.predict(\n",
    "            [target_seq] + states_value, verbose=0\n",
    "        )\n",
    "\n",
    "        # Sample a token\n",
    "        sampled_token_index = np.argmax(output_tokens[0, -1, :])\n",
    "        sampled_char = reverse_target_char_index[sampled_token_index]\n",
    "        decoded_sentence += sampled_char\n",
    "\n",
    "        # Exit condition: either hit max length\n",
    "        # or find stop character.\n",
    "        if sampled_char == \"\\n\" or len(decoded_sentence) > max_decoder_seq_length:\n",
    "            stop_condition = True\n",
    "\n",
    "        # Update the target sequence (of length 1).\n",
    "        target_seq = np.zeros((1, 1, num_decoder_tokens))\n",
    "        target_seq[0, 0, sampled_token_index] = 1.0\n",
    "\n",
    "        # Update states\n",
    "        states_value = [h, c]\n",
    "    return decoded_sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Input word: Go.\n",
      "Output word: Pars !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Run.\n",
      "Output word: Fuyez !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Wait!\n",
      "Output word: Attendez !\n",
      "\n",
      "\n",
      "\n",
      "Input word: I won!\n",
      "Output word: Je vais bon coup.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Attack!\n",
      "Output word: Attaquez !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Got it!\n",
      "Output word: Fais-le !\n",
      "\n",
      "\n",
      "\n",
      "Input word: I paid.\n",
      "Output word: Je payai.\n",
      "\n",
      "\n",
      "\n",
      "Input word: No way!\n",
      "Output word: C'est attaque !\n",
      "\n",
      "\n",
      "\n",
      "Input word: We won.\n",
      "Output word: Nous l'avons emporté.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Be fair.\n",
      "Output word: Soyez crudent !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Beat it.\n",
      "Output word: Firez vetint !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Come in.\n",
      "Output word: Entre.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Fold it.\n",
      "Output word: Plie-le.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Get out!\n",
      "Output word: Dégage !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Go away!\n",
      "Output word: Dégage !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Go home.\n",
      "Output word: Va te cami !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Help me.\n",
      "Output word: Aide-toi à Tom.\n",
      "\n",
      "\n",
      "\n",
      "Input word: How sad!\n",
      "Output word: Comme c'est tris le hemmente.\n",
      "\n",
      "\n",
      "\n",
      "Input word: I tried.\n",
      "Output word: J'ai essayé Tom.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Join me.\n",
      "Output word: Ellaot !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Look up.\n",
      "Output word: Verrouillez-le.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Push it.\n",
      "Output word: Poussez-le.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Skip it.\n",
      "Output word: Parle-toi !\n",
      "\n",
      "\n",
      "\n",
      "Input word: We know.\n",
      "Output word: Nous sûlemmagnes.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Aim high.\n",
      "Output word: Vise plus haut.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Be brief.\n",
      "Output word: Soyez bref.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Fix this.\n",
      "Output word: Répare ça.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Get away!\n",
      "Output word: Dégage !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Get lost!\n",
      "Output word: Dégagez !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Go ahead!\n",
      "Output word: Allez !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Go ahead.\n",
      "Output word: Vas-y !\n",
      "\n",
      "\n",
      "\n",
      "Input word: He spoke.\n",
      "Output word: Il a parlé.\n",
      "\n",
      "\n",
      "\n",
      "Input word: How wise!\n",
      "Output word: Comme c'est tris le hemment !\n",
      "\n",
      "\n",
      "\n",
      "Input word: I got it.\n",
      "Output word: J'ai commencé à Tom.\n",
      "\n",
      "\n",
      "\n",
      "Input word: I use it.\n",
      "Output word: Je suis detrapte.\n",
      "\n",
      "\n",
      "\n",
      "Input word: I'm fair.\n",
      "Output word: Je suis en train de partir.\n",
      "\n",
      "\n",
      "\n",
      "Input word: I'm lazy.\n",
      "Output word: Je suis paresseux.\n",
      "\n",
      "\n",
      "\n",
      "Input word: I'm weak.\n",
      "Output word: Je suis en train de partir.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Keep out!\n",
      "Output word: Gardez-toi !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Look out!\n",
      "Output word: Fais attention.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Run away.\n",
      "Output word: Fuyez.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Speed up.\n",
      "Output word: Laissez-le pas.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Terrific!\n",
      "Output word: Super !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Tom paid.\n",
      "Output word: Tom partir.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Use this.\n",
      "Output word: Utilise ceci.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Who fell?\n",
      "Output word: Qui a parlé ?\n",
      "\n",
      "\n",
      "\n",
      "Input word: Be honest.\n",
      "Output word: Soyez honnête.\n",
      "\n",
      "\n",
      "\n",
      "Input word: Catch him.\n",
      "Output word: Tue-le !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Do it now.\n",
      "Output word: Faites-le !\n",
      "\n",
      "\n",
      "\n",
      "Input word: Fantastic!\n",
      "Output word: Fantastique !\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for seq_index in range(0,1000,20):\n",
    "    input_seq = encoder_input_data[seq_index: seq_index + 1]\n",
    "    decoded_sentence = decode_sequence(input_seq)\n",
    "    print(\"\\n\")\n",
    "    print(f\"Input word: {input_texts[seq_index]}\")\n",
    "    print(f\"Output word: {decoded_sentence}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
